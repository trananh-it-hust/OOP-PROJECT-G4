{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmKXreIfNKQ"
      },
      "source": [
        "# Named entity recognition with Spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download libraries and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Uwm4GYsghhpc",
        "outputId": "da7905b2-c024-4f82-f7ba-40401bd6bacf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import json\n",
        "import tqdm\n",
        "#https://www.newscatcherapi.com/blog/train-custom-named-entity-recognition-ner-model-with-spacy-v3\n",
        "#https://spacy.io/usage/training#quickstart\n",
        "#ner annotator: 'https://tecoholic.github.io/ner-annotator/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merger multiple .json files in resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Doing, don't run this code\n",
        "def merge_json_from_folder(file_paths):\n",
        "    # Iterate over each file path\n",
        "    for file_path in file_paths:\n",
        "        # Check if file exists\n",
        "        if os.path.exists(file_path):\n",
        "            # Read JSON from file\n",
        "            with open(file_path, 'r') as f:\n",
        "                json_data = json.load(f)\n",
        "    # Read contents of both JSON files\n",
        "    with open(file1, 'r', encoding = 'UTF-8') as f1:\n",
        "        json1 = json.load(f1)\n",
        "\n",
        "    with open(file2, 'r', encoding = 'UTF-8') as f2:\n",
        "        json2 = json.load(f2)\n",
        "\n",
        "    text1 = json1.get(\"annotations\", [])[0][0]\n",
        "    text2 = json2.get(\"annotations\", [])[0][0]\n",
        "    entities1 = json1.get(\"annotations\", [])[0][1].get('entities', [])\n",
        "    entities2 = json2.get(\"annotations\", [])[0][1].get('entities', [])\n",
        "    dict1 = {'text': text1, 'entities': entities1}\n",
        "    dict2 = {'text': text2, 'entities': entities2}\n",
        "    Ans = {}\n",
        "    Ans['classes'] = json1.get('classes',[])\n",
        "    Ans['annotations'] = [dict1, dict2]\n",
        "    # Ans['annotations'][0] = {'text': }\n",
        "    # [json1.get(\"annotations\", [])[0], json2.get(\"annotations\",[])[0] ]\n",
        "    #json1[\"annotations\"] = json1.get(\"annotations\", []) + json2.get(\"annotations\",[])\n",
        "    \n",
        "    #merged_json will replace json1\n",
        "    #json1[\"annotations\"] = merged_json\n",
        "\n",
        "    return Ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_json(file1, file2,):\n",
        "    # Read contents of both JSON files\n",
        "    with open(file1, 'r', encoding = 'UTF-8') as f1:\n",
        "        json1 = json.load(f1)\n",
        "\n",
        "    with open(file2, 'r', encoding = 'UTF-8') as f2:\n",
        "        json2 = json.load(f2)\n",
        "\n",
        "    text1 = json1.get(\"annotations\", [])[0][0]\n",
        "    text2 = json2.get(\"annotations\", [])[0][0]\n",
        "    entities1 = json1.get(\"annotations\", [])[0][1].get('entities', [])\n",
        "    entities2 = json2.get(\"annotations\", [])[0][1].get('entities', [])\n",
        "    dict1 = {'text': text1, 'entities': entities1}\n",
        "    dict2 = {'text': text2, 'entities': entities2}\n",
        "    Ans = {}\n",
        "    Ans['classes'] = json1.get('classes',[])\n",
        "    Ans['annotations'] = [dict1, dict2]\n",
        "    # Ans['annotations'][0] = {'text': }\n",
        "    # [json1.get(\"annotations\", [])[0], json2.get(\"annotations\",[])[0] ]\n",
        "    #json1[\"annotations\"] = json1.get(\"annotations\", []) + json2.get(\"annotations\",[])\n",
        "    \n",
        "    #merged_json will replace json1\n",
        "    #json1[\"annotations\"] = merged_json\n",
        "\n",
        "    return Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "jqp49JWanuwJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Terminologies that was created by knowledge of members. PER: Andrew Ng, Steve Jobs, Ralph Merkle, Stuart Haber, W.Scott Stornetta, Satoshi Nakamoto, user, consumer, producer, developer, investor,  leader, Caroline Pham, commissioner, Gary Gensler, CEO. ORG: Oracle, Google, Meta, Apple, Amazon, Viettel, UNICEF, WTO, Multicoin Capital, Commodity Futures Trading Commission. LOC: New York, London, Paris, Tokyo, Shanghai, Peking, HongKong, New South Wales, US, UK, France, Japan, China. PRODUCT: Apps, website, applications. DATE: June 21st, September 15th, 01/01/2023, May 16th 2004, December 12th 2024, 2017, 2024, Oct. 1 . DATA_AI: Machine learning, Data, Artificial intelligence, Deep learning, Bigdata, database, cluster, regression, autoregressive, patterns. BLOCKCHAIN: Blockchain, Crypto currencies, Bitcoin, BTC, Dogecoin, smart contracts, block, hash function, peer-to-peer,P2P consensus mechanism, Cryptography, Distributed ledger, Proof-of-work, Proof-of-stake, Encryption, Digital signature, EVM, Ethereum, decentralization, distributed network, Hyperledger fabric,  Corda, Quorum, BaaS, Blockchain as a Service, Crypto, FTX,  ETF. OTHER_IT_TERMILOGIES: Server, client, network, docker, code, TCP/IP, protocol, bandwidth, cache, Cloud, Cookie, CPU, GPU, CSS, CSP, Cyber-insurance, Cyber Physical Attacks, DDoS attacks, desktop, DevOps, DHCP, DNS, domain, edge computing,  Ethernet, Firewall, gateway, HTTP, IoE, IoT. FINANCE: transactions, contracts, assets, capital gain, balance sheet, capital market, cash flow, depreciation, equity, amortization, trademark, investment,  market, fund, $1.36, venture. CONFERENCE: Blockchain Africa Conference, Blockchain Expo, Europe, European Blockchain Convention, Blockchain Futurist Conference, International Congress on Blockchain and Applications, Blockchain Economy Istanbul, Paris Blockchain Week'"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data = merge_json(r'Training_data\\NER0.json', r'Training_data\\NER1.json' )\n",
        "training_data['annotations'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "xU9_lpA-gX-a"
      },
      "outputs": [],
      "source": [
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin() # create a DocBin object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "6RjBTLtS7Mha"
      },
      "outputs": [],
      "source": [
        "from spacy.util import filter_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 22.40it/s]\n"
          ]
        }
      ],
      "source": [
        "for training_example  in tqdm(training_data['annotations']): \n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text) \n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents \n",
        "    doc_bin.add(doc)\n",
        "\n",
        "doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG06_GE7L_T3",
        "outputId": "b46b8271-68cf-441d-f328-a5319e4a66b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping entity\n"
          ]
        }
      ],
      "source": [
        "text = training_data['annotations']['text']\n",
        "labels = training_data['annotations']['entities']\n",
        "doc = nlp.make_doc(text)\n",
        "ents = []\n",
        "for start, end, label in labels:\n",
        "    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "    if span is None:\n",
        "        print(\"Skipping entity\")\n",
        "    else:\n",
        "        ents.append(span)\n",
        "filtered_ents = filter_spans(ents)\n",
        "doc.ents = filtered_ents\n",
        "doc_bin.add(doc)\n",
        "doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train model by terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogUeByN273Wx",
        "outputId": "aaef8c46-322b-4e5a-dc13-7ca482f6d7eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "TP0Wzzps8S-X",
        "outputId": "8c4edd1c-f2df-4734-e0ff-f051e5b9ed7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    261.57    0.00    0.00    0.00    0.00\n",
            "100     200       1245.09  19990.27  100.00  100.00  100.00    1.00\n",
            "200     400         53.61     34.73  100.00  100.00  100.00    1.00\n",
            "300     600         81.85     29.48  100.00  100.00  100.00    1.00\n",
            "400     800        117.76     42.20  100.00  100.00  100.00    1.00\n",
            "500    1000        184.75     62.49  100.00  100.00  100.00    1.00\n",
            "600    1200        227.47     51.30  100.00  100.00  100.00    1.00\n",
            "700    1400        160.78     35.07  100.00  100.00  100.00    1.00\n",
            "800    1600         66.14     12.17  100.00  100.00  100.00    1.00\n",
            "900    1800        125.67     16.08  100.00  100.00  100.00    1.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test new model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "xxWq49wM-6fq"
      },
      "outputs": [],
      "source": [
        "nlp_ner = spacy.load(\"model-best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', ' Article title', ' Content', ' Creation date', ' Author',\n",
              "       ' Category', ' Tags'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "df = pd.read_csv(r\"D:\\Downloads\\DS&AI\\Team work\\PVT\\OOP-PROJECT-G4\\src\\main\\resources\\data\\preprocessed_data.csv\")\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "cg3K6py5sx63",
        "outputId": "e3a4baa0-d051-4d31-af3b-fa29b3c4148d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'federal reserve digital asset bank custodia access master account judge rule friday federal reserve bank discretion grant master account judge scott skavdahl district court district wyoming judgment news blow custodia bank sue federal reserve board governor federal reserve bank kansas city 2022 delay decision application central bank master account master account allow institution direct access feds payment system provide direct access uss money supply available financial institution master account force rely partner bank master account provide service custodias position correct effectively mean depository institution charter law regardless soundly craft entitle master account allow direct access federal financial system judge accord court document custodia allege federal reserve board violate administrative procedure act action arbitrary capricious abuse discretion otherwise accordance law custodia judge compel board issue master account apa govern federal agency develop issue rule court stamp apa claim board final action master account custodia statutorily entitle master account judge real dispute heart frbkc grant master account custodia legally eligible frbkc possess discretion deny custodias master account application despite eligibility judge custodia file application kansas city feed 2020 spring 2021 federal reserve board governor intervene seek control decision process custodias request master account deny january 2023 custodia immediately respond request comment custodia subject wyoming law special purpose depository institution bank receive deposit custody activity lend customer fiat deposit hold deposit 100 percent reserve accord custodias website'"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[' Content']\n",
        "df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "rJMpTV1Ls64Y",
        "outputId": "616c7ef6-6f86-471f-9267-c197975e1710"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Oracle\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Paris\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "test = input()\n",
        "doc = nlp_ner(test)\n",
        "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "MPKttni9AL44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ORG: \n",
            "(0, 6); \n",
            "LOC: \n",
            "(8, 13); "
          ]
        }
      ],
      "source": [
        "ans = {}\n",
        "for ent in doc.ents:\n",
        "    ans[ent.label_] = []\n",
        "for ent in doc.ents:\n",
        "    ans[ent.label_].append((ent.start_char, ent.end_char))\n",
        "for label in ans: \n",
        "    print(\"\\n\" + label + \": \")\n",
        "    for text in ans[label]: \n",
        "        print(text, end= \"; \")\n",
        "\n",
        "\n",
        "#save to json file \n",
        "file_name = \"Predict_NER3.json\" \n",
        "with open(file_name, 'w') as f: \n",
        "    json.dump(ans,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion: \n",
        "### Results: \n",
        "Model failed successfully. The reasons for that, to my perspective: \n",
        "- The NER0.json file has no meanings. That make hardly any contributions to the training process\n",
        "- Other data is too small\n",
        "### Plans: \n",
        "- About 1st reason, with each tag, we have to find websites full of termilogies about that entity.\n",
        "- Abour 2nd reasion, we should use some LLMs to create training_data for us\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
