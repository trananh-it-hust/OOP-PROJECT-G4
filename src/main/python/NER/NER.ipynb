{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmKXreIfNKQ"
      },
      "source": [
        "# Named entity recognition with Spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download libraries and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Uwm4GYsghhpc",
        "outputId": "da7905b2-c024-4f82-f7ba-40401bd6bacf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import json\n",
        "#https://www.newscatcherapi.com/blog/train-custom-named-entity-recognition-ner-model-with-spacy-v3\n",
        "#https://spacy.io/usage/training#quickstart\n",
        "#ner annotator: 'https://tecoholic.github.io/ner-annotator/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jqp49JWanuwJ"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "with open('NER1.json', 'r') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-eqjryELUWH",
        "outputId": "a40245f7-403b-423c-c95d-7ad150f38502"
      },
      "outputs": [],
      "source": [
        "training_data = {}\n",
        "training_data['classes'] = data['classes']\n",
        "training_data['annotations'] = dict(text = data['annotations'][0][0], entities = data['annotations'][0][1]['entities'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xU9_lpA-gX-a"
      },
      "outputs": [],
      "source": [
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin() # create a DocBin object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6RjBTLtS7Mha"
      },
      "outputs": [],
      "source": [
        "from spacy.util import filter_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG06_GE7L_T3",
        "outputId": "b46b8271-68cf-441d-f328-a5319e4a66b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping entity\n"
          ]
        }
      ],
      "source": [
        "text = training_data['annotations']['text']\n",
        "labels = training_data['annotations']['entities']\n",
        "doc = nlp.make_doc(text)\n",
        "ents = []\n",
        "for start, end, label in labels:\n",
        "    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "    if span is None:\n",
        "        print(\"Skipping entity\")\n",
        "    else:\n",
        "        ents.append(span)\n",
        "filtered_ents = filter_spans(ents)\n",
        "doc.ents = filtered_ents\n",
        "doc_bin.add(doc)\n",
        "doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train model by terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogUeByN273Wx",
        "outputId": "aaef8c46-322b-4e5a-dc13-7ca482f6d7eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "TP0Wzzps8S-X",
        "outputId": "8c4edd1c-f2df-4734-e0ff-f051e5b9ed7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    644.66    0.00    0.00    0.00    0.00\n",
            "200     200       5714.89  21975.95   99.53   99.07  100.00    1.00\n",
            "400     400         30.26     87.75  100.00  100.00  100.00    1.00\n",
            "600     600         32.40      8.50  100.00  100.00  100.00    1.00\n",
            "800     800         13.92      4.14  100.00  100.00  100.00    1.00\n",
            "1000    1000         76.56     15.91  100.00  100.00  100.00    1.00\n",
            "1200    1200        148.79     19.86  100.00  100.00  100.00    1.00\n",
            "1400    1400         99.84     10.51  100.00  100.00  100.00    1.00\n",
            "1600    1600         84.17     17.83  100.00  100.00  100.00    1.00\n",
            "1800    1800        401.50     36.00  100.00  100.00  100.00    1.00\n",
            "2000    2000        467.88     39.73  100.00  100.00  100.00    1.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test new model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xxWq49wM-6fq"
      },
      "outputs": [],
      "source": [
        "nlp_ner = spacy.load(\"model-best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "cg3K6py5sx63",
        "outputId": "e3a4baa0-d051-4d31-af3b-fa29b3c4148d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Introduction This blog post will discuss the step from blockchain generation 3.0 to 4.0 and how DA (Data Analytics) and ML (Machine Learning) can augment blockchain-built solutions. The ﬁrst generation of Blockchain was mainly around the use of basic capabilities and cryptocurrencies or Fintech use, whilst in the second generation, application logic was added in the form of code-based smart contracts to widen its applicability. The third generation was much more about scalability, interoperability and creating good user interfaces to level up with existing business applications. Today, we are looking at generation 4.0, focusing on cross-industry adoption and making enterprise blockchain more usable in real-life business[1]. This raises the question of how more value can be added to these blockchain applications to meet business user expectations. With the growing adoption, the volume of transactions is growing rapidly and a whole new data lake of information is created. Leveraging this lake of timestamped data offers great potential for technologies like DA and ML. As part of an internship at Oracle, the possibilities of the confluence of these technologies have been explored. Figure 1. Evolution of Blockchain in Enterprises. Blockchain augmented with data analytics and machine learning Blockchain has become a technology enabler and gradually serves as the backbone of more and more business processes. As the result, enterprise blockchain applications use DA and ML to create more business value, although each domain may have a slightly different focus. DA can augment blockchain by visualizing the timestamped data in the form of real-time timelines, inventory heat maps, product content charts, supply chain relationships, and other graphs and statistics. These visualizations make it easier to interpret the data on the blockchain ledger and make it more comprehensive. ML on the other hand can be used in a later stage when enough data has been gathered both for training ML models and for its predictions. It can help find interesting patterns, make predictions, look for anomalies and help cluster data. In the future ML might even help to improve the code generation of smart contracts and apply best practices. Although these technologies could supplement each other well, it is still in the early stages. To automate DA, Oracle has already added a specific function to its blockchain offering: ledger data and blockchain operations history is streamed to the Rich History Database in Oracle Blockchain Platform. From the Rich History Database, which is a data schema in Oracle Database synchronized with the ledger’s transactions, transaction payloads and transaction history can be clustered, analyzed and visualized. Figure 2. Rich History Database Off-Chain Synchronization to Oracle Database. Promising industries and use cases Real time examples of promising industries and use cases where Blockchain applications can be augmented with ML/AI and DA are: Health Care: A Blockchain network can help enable secure and patient-authorized data exchange between care provider (e.g, a hospital lab and an external specialist, who needs to access patient’s test results.) Patients can provide consent for access to their medical files, while doctors can share the medical results securely through a permissioned Blockchain. The core capabilities blockchain offers here are control, full traceability, identity, and security. DA is used to visualize the data and show the grouping of patients and doctors per hospital/clinic. Statistics, outliers, and reference lines enhance the visualization of the data. ML could provide predictions about e.g. the occupancy rate of the hospitals/clinics, which would have been an interesting proposition during the COVID-19 pandemic. Figure 3. Data Analytics and Forecasting Using EHR Tracking Blockchain Transaction History Using Oracle Blockchain Platform. Corporate Finance: Blockchain provides a secure way to handle and reconcile invoices between different entities and business units. Blockchain offers a holistic view and full traceability of these invoices. ML can analyze the timestamped data to detect anomalies in real-time and address why invoices were rejected or have taken a much longer route to get paid. DA can be used to visualize the data in real-time with heat maps and Sankey diagrams to show the flows. When using Oracle Analytics it can also generate natural language explanations of the data using AI/ML. This can smoothen the process, generate insights into why certain invoices get rejected more often than others, and eliminate inefficiencies. Figure 4. Timeline View and Natural Language Narrative Generated by Oracle Analytics from Intercompany Financials Reconciliation Using Oracle Blockchain Platform. Figure 5. Cross-Entity Invocing Sankey Diagram Generated by Oracle Analytics from Intercompany Financials Reconciliation Using Oracle Blockchain Platform. Supply Chain: Blockchain can help create a trusted supply chain with authenticated provenance. This creates guaranteed product authenticity for businesses and consumers. Blockchain makes the process faster, offers full traceability based on immutable records, and provides a holistic view across a multi-level supply chain. DA can help visualize the timestamped data in an immutable supply chain timeline and this can even be accessed on the end-product using a QR code that customers could scan. Customers could see product composition and the end-to-end sourcing across multiple suppliers, verify sustainability claims, drill down into source of recycled content, and much more. This is not only helpful for sustainability management, as customers and businesses are ensured that a product is truly sustainable, but helps to increase retailer’s business as case studies indicate that when customers can access such data they are more likely to buy from these retailers[2]. ML can also help optimize supply chain routes and predict where potential bottlenecks or disruptions could arise. Figure 6. Example of Product Composition Drill Down and Products/Companies Relationships Generated by Oracle Analytics from Oracle Blockchain Platform Transaction History in Textile/Clothing Manufacturing. Conclusion We believe there is more value to Blockchain applications when it is augmented with ML and DA, especially with the transition to Blockchain 4.0. Due to the disruptive nature of blockchain in terms of business processes, it is recommended to ensure organizations get the Blockchain concept first, before blockchain applications are enhanced with these technologies. The Rich History Database offers the tools to support the confluence of these technologies, starting with the visualization of the ledger’s data. When enough data has been gathered ML models and tools built-in within Oracle Database can be used to further augment the application discovering patterns, trends or anomalies. Consider the right order, but early adaptors will accelerate the value delivered from their Blockchain solutions as DA and ML complement Blockchain 4.0 capabilities. [1] www.researchgate.net/publication/351263701_Blockchain_10_to_Blockchain_40-The_Evolutionary_Transformation_of_Blockchain_Technology [2] https://www.retraced.com/en/news/client-success-story-boyish-x-retraced-case-study'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(r\"D:\\Eclipse\\sele-java\\blockchain.csv\")\n",
        "df = df['Content']\n",
        "df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "rJMpTV1Ls64Y",
        "outputId": "616c7ef6-6f86-471f-9267-c197975e1710"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "test = df[3]\n",
        "doc = nlp_ner(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MPKttni9AL44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BLOCKCHAIN_TECH: \n",
            "(274, 281); (409, 419); (691, 701); (1292, 1296); (1310, 1321); (2157, 2164); (2493, 2497); (2640, 2650); (2790, 2800); (3072, 3082); (3462, 3472); (3507, 3517); (4367, 4377); (5154, 5164); (5403, 5413); (5563, 5568); (5795, 5800); \n",
            "PERSON: \n",
            "(349, 369); (1641, 1645); (1907, 1911); (2083, 2092); (3240, 3248); (4846, 4859); \n",
            "DATA_AI: \n",
            "(1049, 1053); (1554, 1561); (2530, 2544); (2903, 2914); (3610, 3615); (3616, 3623); (3625, 3633); (3690, 3697); (4245, 4254); (6027, 6041); \n",
            "ORDINAL: \n",
            "(3821, 3826); "
          ]
        }
      ],
      "source": [
        "ans = {}\n",
        "for ent in doc.ents:\n",
        "    ans[ent.label_] = []\n",
        "for ent in doc.ents:\n",
        "    ans[ent.label_].append((ent.start_char, ent.end_char))\n",
        "for label in ans: \n",
        "    print(\"\\n\" + label + \": \")\n",
        "    for text in ans[label]: \n",
        "        print(text, end= \"; \")\n",
        "\n",
        "\n",
        "#save to json file \n",
        "file_name = \"Predict_NER3.json\" \n",
        "with open(file_name, 'w') as f: \n",
        "    json.dump(ans,f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
